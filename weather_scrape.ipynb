{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "from selenium import webdriver\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of desired months to scrape\n",
    "dates = [\"2008-01\", \"2021-01\"]\n",
    "start, end = [datetime.strptime(_, '%Y-%m') for _ in dates]\n",
    "month_ordereddict = OrderedDict(((start + timedelta(_)).strftime(\"%Y-%-m\"), None) for _ in range((end - start).days)).keys()\n",
    "month_list = list(month_ordereddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping weather data for five large cities in state - all in different geographical areas\n",
    "#url string for each city\n",
    "kenner = \"KMSY/date/\"\n",
    "baton_rouge = \"KBTR/date/\"\n",
    "lake_charles = \"KLCH/date/\"\n",
    "alexandria = \"KAEX/date/\"\n",
    "shreveport = \"KSHV/date/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [kenner, baton_rouge, lake_charles, alexandria, shreveport]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_site_base_url = \"https://www.wunderground.com/history/monthly/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append string for city url\n",
    "url_cities = []\n",
    "for city in cities:\n",
    "    url_cities.append(weather_site_base_url + city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append string for month url\n",
    "urls = []\n",
    "for url_city in url_cities:\n",
    "    for month in month_list:\n",
    "        urls.append(url_city + month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create headers for resulting table, post-scrape\n",
    "results_header_row_vals = [\n",
    "    \"ID\",\n",
    "    \"max_temp_deg_f\",\n",
    "    \"avg_temp_deg_f\",\n",
    "    \"min_temp_deg_f\",\n",
    "    \"max_dew_point_deg_f\",\n",
    "    \"avg_dew_point_deg_f\",\n",
    "    \"min_dew_point_deg_f\",\n",
    "    \"max_dew_humidity_pct\",\n",
    "    \"avg_dew_humidity_pct\",\n",
    "    \"min_dew_humidity_pct\",\n",
    "    \"max_wind_speed_mph\",\n",
    "    \"avg_wind_speed_mph\",\n",
    "    \"min_wind_speed_mph\",\n",
    "    \"max_pressure_hg\",\n",
    "    \"avg_pressure_hg\",\n",
    "    \"min_pressure_hg\",\n",
    "    \"tot_precipitation_in\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#perform scrape for each city, scraped seperately to avoid losing data if error raised\n",
    "kenner_urls = []\n",
    "for url in urls:\n",
    "    if kenner in url:\n",
    "        kenner_urls.append(url)\n",
    "\n",
    "kenner_list = []\n",
    "\n",
    "for url in kenner_urls:\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(.5+2*random.random())\n",
    "\n",
    "    monthly_data_page_html_text = driver.page_source\n",
    "    soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "    html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "    \n",
    "    \n",
    "    try: #troubleshooting attributeerror occasionally raised\n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    except AttributeError:\n",
    "        driver.get(url)\n",
    "    \n",
    "        time.sleep(.5+2*random.random())\n",
    "\n",
    "        monthly_data_page_html_text = driver.page_source\n",
    "        soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "        html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "        \n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    \n",
    "    \n",
    "    html_table_body = html_table.findChild(\"tbody\")\n",
    "\n",
    "    html_table_columns = html_table_body.findChild(\"tr\").findChildren(\"td\", recursive=False) \n",
    "\n",
    "    num_rows = len(html_table_columns[0].find_all(\"tr\"))\n",
    "    row_index = 1\n",
    "\n",
    "    while row_index < num_rows:\n",
    "\n",
    "        new_row = []\n",
    "\n",
    "        for html_column in html_table_columns:\n",
    "\n",
    "            html_column_rows = html_column.find_all(\"tr\")\n",
    "            html_current_column_row = html_column_rows[row_index]\n",
    "\n",
    "            html_current_column_cells = html_current_column_row.findChildren(\"td\", recursive=False)\n",
    "\n",
    "            for html_cell in html_current_column_cells:\n",
    "\n",
    "                is_first_cell_in_row = len(new_row) == 0\n",
    "\n",
    "                html_cell_content = html_cell.contents[0].strip()\n",
    "\n",
    "                if is_first_cell_in_row:\n",
    "                    result_cell_content = '_'.join(url.rsplit('/', 3)[1::2])\n",
    "                else:\n",
    "                    result_cell_content = html_cell_content\n",
    "\n",
    "                new_row.append(result_cell_content)\n",
    "\n",
    "        kenner_list.append(new_row)\n",
    "\n",
    "        row_index += 1\n",
    "\n",
    "kenner_scrape = pd.DataFrame(kenner_list, columns=results_header_row_vals)\n",
    "kenner_scrape.to_pickle('kenner_scrape.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform scrape for each city, scraped seperately to avoid losing data if error raised\n",
    "baton_rouge_urls = []\n",
    "for url in urls:\n",
    "    if baton_rouge in url:\n",
    "        baton_rouge_urls.append(url)\n",
    "\n",
    "baton_rouge_list = []\n",
    "\n",
    "for url in baton_rouge_urls:\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(.5+2*random.random())\n",
    "\n",
    "    monthly_data_page_html_text = driver.page_source\n",
    "    soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "    html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "    \n",
    "    \n",
    "    try: #troubleshooting attributeerror occasionally raised\n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    except AttributeError:\n",
    "        driver.get(url)\n",
    "    \n",
    "        time.sleep(.5+2*random.random())\n",
    "\n",
    "        monthly_data_page_html_text = driver.page_source\n",
    "        soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "        html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "        \n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    \n",
    "    \n",
    "    html_table_body = html_table.findChild(\"tbody\")\n",
    "\n",
    "    html_table_columns = html_table_body.findChild(\"tr\").findChildren(\"td\", recursive=False) \n",
    "\n",
    "    num_rows = len(html_table_columns[0].find_all(\"tr\"))\n",
    "    row_index = 1\n",
    "\n",
    "    while row_index < num_rows:\n",
    "\n",
    "        new_row = []\n",
    "\n",
    "        for html_column in html_table_columns:\n",
    "\n",
    "            html_column_rows = html_column.find_all(\"tr\")\n",
    "            html_current_column_row = html_column_rows[row_index]\n",
    "\n",
    "            html_current_column_cells = html_current_column_row.findChildren(\"td\", recursive=False)\n",
    "\n",
    "            for html_cell in html_current_column_cells:\n",
    "\n",
    "                is_first_cell_in_row = len(new_row) == 0\n",
    "\n",
    "                html_cell_content = html_cell.contents[0].strip()\n",
    "\n",
    "                if is_first_cell_in_row:\n",
    "                    result_cell_content = '_'.join(url.rsplit('/', 3)[1::2])\n",
    "                else:\n",
    "                    result_cell_content = html_cell_content\n",
    "\n",
    "                new_row.append(result_cell_content)\n",
    "\n",
    "        baton_rouge_list.append(new_row)\n",
    "\n",
    "        row_index += 1\n",
    "\n",
    "baton_rouge_scrape = pd.DataFrame(baton_rouge_list, columns=results_header_row_vals)\n",
    "\n",
    "baton_rouge_scrape.to_pickle('baton_rouge_scrape.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform scrape for each city, scraped seperately to avoid losing data if error raised\n",
    "lake_charles_urls = []\n",
    "for url in urls:\n",
    "    if lake_charles in url:\n",
    "        lake_charles_urls.append(url)\n",
    "\n",
    "lake_charles_list = []\n",
    "\n",
    "for url in lake_charles_urls:\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(.5+2*random.random())\n",
    "\n",
    "    monthly_data_page_html_text = driver.page_source\n",
    "    soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "    html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "    \n",
    "    \n",
    "    try: #troubleshooting attributeerror occasionally raised\n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    except AttributeError:\n",
    "        driver.get(url)\n",
    "    \n",
    "        time.sleep(.5+2*random.random())\n",
    "\n",
    "        monthly_data_page_html_text = driver.page_source\n",
    "        soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "        html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "        \n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    \n",
    "    \n",
    "    html_table_body = html_table.findChild(\"tbody\")\n",
    "\n",
    "    html_table_columns = html_table_body.findChild(\"tr\").findChildren(\"td\", recursive=False) \n",
    "\n",
    "    num_rows = len(html_table_columns[0].find_all(\"tr\"))\n",
    "    row_index = 1\n",
    "\n",
    "    while row_index < num_rows:\n",
    "\n",
    "        new_row = []\n",
    "\n",
    "        for html_column in html_table_columns:\n",
    "\n",
    "            html_column_rows = html_column.find_all(\"tr\")\n",
    "            html_current_column_row = html_column_rows[row_index]\n",
    "\n",
    "            html_current_column_cells = html_current_column_row.findChildren(\"td\", recursive=False)\n",
    "\n",
    "            for html_cell in html_current_column_cells:\n",
    "\n",
    "                is_first_cell_in_row = len(new_row) == 0\n",
    "\n",
    "                html_cell_content = html_cell.contents[0].strip()\n",
    "\n",
    "                if is_first_cell_in_row:\n",
    "                    result_cell_content = '_'.join(url.rsplit('/', 3)[1::2])\n",
    "                else:\n",
    "                    result_cell_content = html_cell_content\n",
    "\n",
    "                new_row.append(result_cell_content)\n",
    "\n",
    "        lake_charles_list.append(new_row)\n",
    "\n",
    "        row_index += 1\n",
    "\n",
    "lake_charles_scrape = pd.DataFrame(lake_charles_list, columns=results_header_row_vals)\n",
    "\n",
    "lake_charles_scrape.to_pickle('lake_charles_scrape.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform scrape for each city, scraped seperately to avoid losing data if error raised\n",
    "alexandria_urls = [] \n",
    "for url in urls:\n",
    "    if alexandria in url:\n",
    "        alexandria_urls.append(url)\n",
    "\n",
    "alexandria_list = []\n",
    "\n",
    "for url in alexandria_urls:\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(.5+2*random.random())\n",
    "\n",
    "    monthly_data_page_html_text = driver.page_source\n",
    "    soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "    html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "    \n",
    "    \n",
    "    try: #troubleshooting attributeerror occasionally raised\n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    except AttributeError:\n",
    "        driver.get(url)\n",
    "    \n",
    "        time.sleep(.5+2*random.random())\n",
    "\n",
    "        monthly_data_page_html_text = driver.page_source\n",
    "        soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "        html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "        \n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    \n",
    "    \n",
    "    html_table_body = html_table.findChild(\"tbody\")\n",
    "\n",
    "    html_table_columns = html_table_body.findChild(\"tr\").findChildren(\"td\", recursive=False) \n",
    "\n",
    "    num_rows = len(html_table_columns[0].find_all(\"tr\"))\n",
    "    row_index = 1\n",
    "\n",
    "    while row_index < num_rows:\n",
    "\n",
    "        new_row = []\n",
    "\n",
    "        for html_column in html_table_columns:\n",
    "\n",
    "            html_column_rows = html_column.find_all(\"tr\")\n",
    "            html_current_column_row = html_column_rows[row_index]\n",
    "\n",
    "            html_current_column_cells = html_current_column_row.findChildren(\"td\", recursive=False)\n",
    "\n",
    "            for html_cell in html_current_column_cells:\n",
    "\n",
    "                is_first_cell_in_row = len(new_row) == 0\n",
    "\n",
    "                html_cell_content = html_cell.contents[0].strip()\n",
    "\n",
    "                if is_first_cell_in_row:\n",
    "                    result_cell_content = '_'.join(url.rsplit('/', 3)[1::2])\n",
    "                else:\n",
    "                    result_cell_content = html_cell_content\n",
    "\n",
    "                new_row.append(result_cell_content)\n",
    "\n",
    "        alexandria_list.append(new_row)\n",
    "\n",
    "        row_index += 1\n",
    "\n",
    "alexandria_scrape = pd.DataFrame(alexandria_list, columns=results_header_row_vals)\n",
    "\n",
    "alexandria_scrape.to_pickle('alexandria_scrape.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform scrape for each city, scraped seperately to avoid losing data if error raised\n",
    "shreveport_urls = []\n",
    "for url in urls:\n",
    "    if shreveport in url:\n",
    "        shreveport_urls.append(url)\n",
    "\n",
    "shreveport_list = []\n",
    "\n",
    "for url in shreveport_urls:\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(.5+2*random.random())\n",
    "\n",
    "    monthly_data_page_html_text = driver.page_source\n",
    "    soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "    html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "    \n",
    "    \n",
    "    try: #troubleshooting attributeerror occasionally raised\n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    except AttributeError:\n",
    "        driver.get(url)\n",
    "    \n",
    "        time.sleep(.5+2*random.random())\n",
    "\n",
    "        monthly_data_page_html_text = driver.page_source\n",
    "        soup = BeautifulSoup(monthly_data_page_html_text, 'html.parser')\n",
    "\n",
    "        html_table_container = soup.find(attrs={\"class\": \"observation-table\"})\n",
    "        \n",
    "        html_table = html_table_container.findChild(\"table\")\n",
    "    \n",
    "    \n",
    "    html_table_body = html_table.findChild(\"tbody\")\n",
    "\n",
    "    html_table_columns = html_table_body.findChild(\"tr\").findChildren(\"td\", recursive=False) \n",
    "\n",
    "    num_rows = len(html_table_columns[0].find_all(\"tr\"))\n",
    "    row_index = 1\n",
    "\n",
    "    while row_index < num_rows:\n",
    "\n",
    "        new_row = []\n",
    "\n",
    "        for html_column in html_table_columns:\n",
    "\n",
    "            html_column_rows = html_column.find_all(\"tr\")\n",
    "            html_current_column_row = html_column_rows[row_index]\n",
    "\n",
    "            html_current_column_cells = html_current_column_row.findChildren(\"td\", recursive=False)\n",
    "\n",
    "            for html_cell in html_current_column_cells:\n",
    "\n",
    "                is_first_cell_in_row = len(new_row) == 0\n",
    "\n",
    "                html_cell_content = html_cell.contents[0].strip()\n",
    "\n",
    "                if is_first_cell_in_row:\n",
    "                    result_cell_content = '_'.join(url.rsplit('/', 3)[1::2])\n",
    "                else:\n",
    "                    result_cell_content = html_cell_content\n",
    "\n",
    "                new_row.append(result_cell_content)\n",
    "\n",
    "        shreveport_list.append(new_row)\n",
    "\n",
    "        row_index += 1\n",
    "\n",
    "shreveport_scrape = pd.DataFrame(shreveport_list, columns=results_header_row_vals)\n",
    "\n",
    "shreveport_scrape.to_pickle('shreveport_scrape.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
